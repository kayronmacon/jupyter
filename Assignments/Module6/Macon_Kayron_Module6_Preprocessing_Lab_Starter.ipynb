{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 6 – Preprocessing Lab Notebook\n",
    "## Preparing Data for Machine Learning\n",
    "\n",
    "### Objectives:\n",
    "- Clean a cybersecurity dataset\n",
    "- Encode categorical variables\n",
    "- Scale numerical features\n",
    "- Perform train-test split\n",
    "- Document each preprocessing step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import textwrap\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df = \u001b[43mpd\u001b[49m.read_csv(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33mC:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mUsers\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mkayro\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mjupyter\u001b[39m\u001b[33m\\\u001b[39m\u001b[33massignments\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mModule6\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mmodule6_cyber_ml_dataset.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      2\u001b[39m df.head()\n\u001b[32m      3\u001b[39m df.info()\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\kayro\\jupyter\\assignments\\Module6\\module6_cyber_ml_dataset.csv')\n",
    "df.head()\n",
    "df.info()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Check for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset is complete with no missing values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timestamp         0\n",
       "SourceIP          0\n",
       "DestinationIP     0\n",
       "Protocol          0\n",
       "BytesSent         0\n",
       "BytesReceived     0\n",
       "LoginAttempts     0\n",
       "Severity Level    0\n",
       "AttackType        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary=f\"\"\"dataset is complete with no missing values\"\"\"\n",
    "print(textwrap.fill(summary, width=80))\n",
    "df.isnull().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Encode Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'textwrap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m summary=\u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[33mvalues are encoded into numbers because machine learning models cannot do math on text\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtextwrap\u001b[49m.fill(summary, width=\u001b[32m80\u001b[39m))\n\u001b[32m      4\u001b[39m encoder = LabelEncoder()\n\u001b[32m      5\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mProtocol\u001b[39m\u001b[33m'\u001b[39m] = encoder.fit_transform(df[\u001b[33m'\u001b[39m\u001b[33mProtocol\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'textwrap' is not defined"
     ]
    }
   ],
   "source": [
    "summary=f\"\"\"values are encoded into numbers because machine learning models cannot do math on text\\n\"\"\"\n",
    "print(textwrap.fill(summary, width=80))\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "df['Protocol'] = encoder.fit_transform(df['Protocol'])\n",
    "print(dict(zip(encoder.classes_, range(len(encoder.classes_)))))\n",
    "df['AttackType'] = encoder.fit_transform(df['AttackType'])\n",
    "\n",
    "print(dict(zip(encoder.classes_, range(len(encoder.classes_)))))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Step 4: Feature Engineering\n\nFeature engineering creates new columns by transforming or combining existing variables. This gives the model stronger, more informative signals without collecting any new data. The two features below are built before scaling so they get normalized along with the rest of the numeric columns.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Feature 1: TotalBytes\n\n**What it represents:**\n`TotalBytes` is the sum of `BytesSent` and `BytesReceived` for a single network event. It captures the full volume of data exchanged in both directions during one connection.\n\n**Why it improves predictive signal:**\n`BytesSent` and `BytesReceived` individually only tell half the story. A model using both raw columns has to learn their combined effect on its own. By pre-computing the total, we give the model a direct measure of overall traffic size — a single number that correlates strongly with bandwidth-heavy attacks — reducing the work the model has to do.\n\n**How it supports cybersecurity analysis:**\nAttacks like DDoS and data exfiltration generate unusually high total traffic. A large `TotalBytes` value is a quick flag that something abnormal may be happening on a connection, making it one of the most practical features for network anomaly detection.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Feature 1: TotalBytes — total traffic volume per event\ndf['TotalBytes'] = df['BytesSent'] + df['BytesReceived']\n\nprint('TotalBytes sample values:')\nprint(df['TotalBytes'].describe())\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Feature 2: ByteRatio\n\n**What it represents:**\n`ByteRatio` is the ratio of bytes sent to bytes received (`BytesSent / (BytesReceived + 1)`). The `+1` prevents division by zero on rows where nothing was received. A value near 1.0 means traffic was balanced; a very high value means far more was sent than received, and vice versa.\n\n**Why it improves predictive signal:**\nRaw byte counts alone do not capture the *direction* imbalance of a connection. Two events can have the same `TotalBytes` but opposite traffic patterns. `ByteRatio` encodes that asymmetry as a single interaction term, giving the model a feature that neither `BytesSent` nor `BytesReceived` can express on its own.\n\n**How it supports cybersecurity analysis:**\nTraffic direction imbalance is a hallmark of specific attack types. Brute-force login attempts send many small requests (high `ByteRatio`), while data exfiltration uploads large payloads outbound (also high `ByteRatio`). A DDoS flood may show the opposite pattern — massive inbound traffic with little sent. `ByteRatio` helps the model separate these attack signatures.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Feature 2: ByteRatio — traffic direction imbalance (+1 avoids division by zero)\ndf['ByteRatio'] = df['BytesSent'] / (df['BytesReceived'] + 1)\n\nprint('ByteRatio sample values:')\nprint(df['ByteRatio'].describe())\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Verify engineered features — check for missing values or infinite numbers\nprint('Null values in engineered features:')\nprint(df[['TotalBytes', 'ByteRatio']].isnull().sum())\n\nprint('\\nInfinite values in ByteRatio:', np.isinf(df['ByteRatio']).sum())\n\nprint('\\nEngineered features preview:')\ndf[['BytesSent', 'BytesReceived', 'TotalBytes', 'ByteRatio']].head(10)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 5: Feature Scaling",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "summary = \"\"\"Scalers look at each numeric column and calculate its mean and standard deviation.\nThis normalizes the data, putting all numeric columns on the same scale.\nThe two engineered features (TotalBytes and ByteRatio) are included here so they\nare scaled consistently with the rest of the dataset.\"\"\"\nprint(textwrap.fill(summary, width=80))\n\nscaler = StandardScaler()\nnumeric_cols = ['BytesSent', 'BytesReceived', 'LoginAttempts', 'Severity Level',\n                'TotalBytes', 'ByteRatio']\ndf[numeric_cols] = scaler.fit_transform(df[numeric_cols])\ndf.head()"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Step 6: Train-Test Split"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.conda (Python 3.11.14)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n .conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "summary=\"\"\"you never want to test a model on data it was trained on, this split ensures the machine learning model is being tested\n",
    "on a significantly larger set of of new data than its training data\"\"\"\n",
    "print(textwrap.fill(summary, width=80))\n",
    "\n",
    "X = df.drop('AttackType', axis=1)\n",
    "y = df['AttackType']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print('Training set size:', X_train.shape)\n",
    "print('Testing set size:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection Questions\n",
    "1. Why is encoding necessary before modeling?\n",
    "2. Why should scaling occur after splitting data?\n",
    "3. How does preprocessing impact model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Why is encoding necessary before modeling?\n",
    "Your Answer: Values are encoded into numbers because its much easier for machine learning models to do math on numbers rather than text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Why should scaling occur after splitting data?\n",
    "your Answer: This normalizes the data, so all different numbers are on a similar scale. This prevents outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. How does preprocessing impact model performance?\n",
    "Your Answer: Makes table data computable, ensuring values are evaluated equally, and removing null instances that can throw off machine learning models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}